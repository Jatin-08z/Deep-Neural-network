{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from testCases_v4a import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buidling the Helper functions for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):  # for a two layer Neural Network\n",
    "    \n",
    "   \n",
    "    W1 = np.random.randn(n_h, n_x)\n",
    "    b1 = np.zeros((n_h,1))\n",
    "   \n",
    "    W2 = np.random.randn(n_y, n_h)\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h,1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y,1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n",
      "   1.74481176 -0.7612069   0.3190391  -0.24937038  1.46210794 -2.06014071]\n",
      " [-0.3224172  -0.38405435  1.13376944 -1.09989127 -0.17242821 -0.87785842\n",
      "   0.04221375  0.58281521 -1.10061918  1.14472371  0.90159072  0.50249434]\n",
      " [ 0.90085595 -0.68372786 -0.12289023 -0.93576943 -0.26788808  0.53035547\n",
      "  -0.69166075 -0.39675353 -0.6871727  -0.84520564 -0.67124613 -0.0126646 ]\n",
      " [-1.11731035  0.2344157   1.65980218  0.74204416 -0.19183555 -0.88762896\n",
      "  -0.74715829  1.6924546   0.05080775 -0.63699565  0.19091548  2.10025514]\n",
      " [ 0.12015895  0.61720311  0.30017032 -0.35224985 -1.1425182  -0.34934272\n",
      "  -0.20889423  0.58662319  0.83898341  0.93110208  0.28558733  0.88514116]]\n",
      "b1[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2[[-0.75439794  1.25286816  0.51292982 -0.29809284  0.48851815]]\n",
      "b2[[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(12, 5, 1)\n",
    "\n",
    "print(\"W1\" + str(parameters[\"W1\"]))\n",
    "print(\"b1\" + str(parameters[\"b1\"]))\n",
    "print(\"W2\" + str(parameters[\"W2\"]))\n",
    "print(\"b2\" + str(parameters[\"b2\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims): # initializing parameters for a L layer deep neural network\n",
    "\n",
    "    L = len(layer_dims)  #layer in the network.\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        \n",
    "        parameters[\"W\"+ str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        parameters[\"b\"+ str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        \n",
    "        assert( parameters[\"W\"+ str(l)].shape ==  ( layer_dims[l], layer_dims[l-1]))\n",
    "        assert( parameters[\"b\"+ str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.07557171  1.13162939]\n",
      " [ 1.51981682  2.18557541]\n",
      " [-1.39649634 -1.44411381]] \n",
      "\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "\n",
      "W2 = [[-0.50446586  0.16003707  0.87616892]\n",
      " [ 0.31563495 -2.02220122 -0.30620401]\n",
      " [ 0.82797464  0.23009474  0.76201118]\n",
      " [-0.22232814 -0.20075807  0.18656139]] \n",
      "\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "\n",
      "W3 = [[ 0.41005165  0.19829972  0.11900865 -0.67066229]\n",
      " [ 0.37756379  0.12182127  1.12948391  1.19891788]\n",
      " [ 0.18515642 -0.37528495 -0.63873041  0.42349435]\n",
      " [ 0.07734007 -0.34385368  0.04359686 -0.62000084]\n",
      " [ 0.69803203 -0.44712856  1.2245077   0.40349164]\n",
      " [ 0.59357852 -1.09491185  0.16938243  0.74055645]\n",
      " [-0.9537006  -0.26621851  0.03261455 -1.37311732]\n",
      " [ 0.31515939  0.84616065 -0.85951594  0.35054598]\n",
      " [-1.31228341 -0.03869551 -1.61577235  1.12141771]] \n",
      "\n",
      "b3 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([2,3,4,9])\n",
    "\n",
    "for l in range(1, 4):\n",
    "    \n",
    "    print(\"W\"+str(l)+ \" = \" + str(parameters[\"W\" + str(l)]), '\\n')\n",
    "    print(\"b\"+str(l)+ \" = \" + str(parameters[\"b\" + str(l)]), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if a vector of an intermediate_variable( of the computation graph) is computed for all training examples, then to make sense of it on a smaller scale try making sense of each column of the vector.Each column contains data about a single image and is easlier to comprehend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Forward.\n",
    "\n",
    "Linear forward modeule computes $$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$ also called pre-activation parameter ( vectorised over all training examples )\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    assert( Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    cache  = (A_prev, W, b)   # cache is to give some info about the current Z.\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  9]\n",
      " [ 9 11]]\n",
      "\n",
      "[[ 4  9]\n",
      " [ 9 11]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.choice(10,(2,2)) + 2\n",
    "ans, _ = relu(z)\n",
    "print(z, end = '\\n'+'\\n')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z  [[ 3.26295337 -1.23429987]] \n",
      "\n",
      "cache  (array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862],\n",
      "       [ 0.86540763, -2.3015387 ]]), array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]), array([[-0.24937038]])) \n",
      "\n",
      "cache[0] is A_prev used to calculate Z \n",
      " cache[1] and cache[2] are W and b for current layer respectively\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "Z, cache = linear_forward(A, W, b)\n",
    "\n",
    "print(\"Z  \" + str(Z),'\\n')\n",
    "print(\"cache  \" + str(cache),'\\n')   #try to make some sense of the cache here\n",
    "\n",
    "print(\"cache[0] is A_prev used to calculate Z\",'\\n', \"cache[1] and cache[2] are W and b for current layer respectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation applied on pre-activation_parameter Z , called as activation forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_forward(Z, activation):\n",
    "    \n",
    "    A = np.zeros((Z.shape[0], Z.shape[1]))\n",
    "    \n",
    "    if (activation == \"sigmoid\"):\n",
    "        A, activation_cache = sigmoid(Z) # Sigmoid function adds its input argument as its cache.\n",
    "    elif (activation == \"relu\"):\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    return A, activation_cache # this activation_cache has just Z stored in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear_activation_forward\n",
    "\n",
    "Implement the forward propagation for the LINEAR->ACTIVATION layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tan_h(Z):\n",
    "    \n",
    "    A = np.tanh(Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    Z , linear_cache = linear_forward(A_prev, W, b)\n",
    "    assert( Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "   \n",
    "    if activation == \"sigmoid\":\n",
    "    \n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\" :\n",
    "        \n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        A, activation_cache = tan_h(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)  #note that cache stores both linear and activation_cache.\n",
    "    \n",
    "    return A, cache \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A =  [[0.96890023 0.11013289]]\n",
      "With relu: A =  [[3.43896131 0.        ]]\n",
      "With tanh: A =  [[ 0.99794156 -0.96982745]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "print(\"With sigmoid: A =  \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "print(\"With relu: A =  \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, \"tanh\")\n",
    "print(\"With tanh: A =  \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The [linear->activation] computation is counted as a single layer not two layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Layer model\n",
    "\n",
    "##### Implementing forward propagation for the whole model\n",
    "\n",
    "We will create a function that will replicate [Linear-relu] L-1 times followed by [Linear-sigmoid] computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters): #the aim of this function is to compute AL i.e activations of the last layer.\n",
    "    \n",
    "    caches = []\n",
    "    L = len(parameters) //2 #layer\n",
    "    \n",
    "    A_prev = X #initialising forward propagation.\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        \n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "     \n",
    "        A, linear_activation_cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "        caches.append(linear_activation_cache)\n",
    "        \n",
    "        A_prev = A\n",
    "     \n",
    "    WL = parameters[\"W\" + str(L)]\n",
    "    bL = parameters[\"b\" + str(L)]\n",
    "    AL ,linear_activation_cache = linear_activation_forward(A_prev, WL, bL, \"sigmoid\")\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "    caches.append(linear_activation_cache)\n",
    "    \n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches lsit: 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden() #3 layer nn test_case.\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "print(\"AL \" + str(AL))\n",
    "print(\"Length of caches lsit: \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sense of caches list. #complicated case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost\n",
    "\n",
    "Formula Used: $ $ $$ J(W, b) = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = - np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL)) / m\n",
    "    \n",
    "    np.squeeze(cost)\n",
    "    assert( cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_cost_test_case' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1581ffdc5f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost_test_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cost :\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_cost_test_case' is not defined"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "print(\"cost :\" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache):\n",
    "    \n",
    "    #unpacking values from linear_cache\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW      = np.dot(dZ, A_prev.T) / m\n",
    "    db      = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "dW, db, dA_prev = linear_backward(dZ, linear_cache)\n",
    "\n",
    "print(\"dA_prev \"+ str(dA_prev))\n",
    "print(\"dW \"+ str(dW))\n",
    "print(\"db \"+ str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    #unpack values from cache into linear and activation cache\n",
    "    linear_cache, activation_cache = cache\n",
    "   \n",
    "    Z = activation_cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        \n",
    "        dZ      = sigmoid_backward(dA, Z)\n",
    "        assert(dZ.shape == Z.shape)\n",
    "        \n",
    "        dW, db, dA_prev  = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        \n",
    "        dZ      = relu_backward(dA, Z)\n",
    "        dW, db, dA_prev  = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with sigmoid: \n",
      "dA_prev[[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW[[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db[[-0.05729622]]\n",
      "\n",
      "with relu: \n",
      "dA_prev[[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW[[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db[[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, \"sigmoid\")\n",
    "print(\"with sigmoid: \")\n",
    "print(\"dA_prev\"+ str(dA_prev))\n",
    "print(\"dW\"+ str(dW))\n",
    "print(\"db\"+ str(db)+'\\n')\n",
    "\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, \"relu\")\n",
    "print(\"with relu: \")\n",
    "print(\"dA_prev\"+ str(dA_prev))\n",
    "print(\"dW\"+ str(dW))\n",
    "print(\"db\"+ str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    #this function does [L-1]*[Linear_relu_backward]<-[Linear_sigmoid_backward]\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # each value is a tuple containing linear_cache and activation_cache for a layer.\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    print(dAL)\n",
    "    assert(dAL.shape == AL.shape)\n",
    "    \n",
    "    current_cache = cache[L-1]\n",
    "    dA_prev, dW_temp, db_temp = linear_activation_backward(dAL ,caches[L-1], \"sigmoid\")\n",
    "    \n",
    "    grads[\"dA\"+ str(L-1)] = dA_prev\n",
    "    grads[\"dW\"+ str(L)] = dW_temp\n",
    "    grads[\"db\"+ str(L)] = db_temp\n",
    "    \n",
    "\n",
    "    for l in reversed(range(L-1)): # L-1 index has cache values for Lth layer, u hve 2 start from L-1th layer.\n",
    "        \n",
    "        current_cache = caches[l] #caches for current layer.\n",
    "        dA_prev, dW_temp, db_temp = linear_activation_backward(dA_prev, current_cache, \"relu\")\n",
    "        \n",
    "        # l+1 will give us the right number for the layer\n",
    "        grads[\"dA\"+str(l)]   = dA_prev\n",
    "        grads[\"dW\"+str(l+1)] = dW_temp\n",
    "        grads[\"db\"+str(l+1)] = db_temp\n",
    "        \n",
    "    #the resultant dA_prev_temp contains gradients wrt input features( grads wrt to the activations of the layer 0)\n",
    "    #they are not required in supervised learning.\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5590876   1.77465392]]\n",
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) //2 # layers\n",
    "    \n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"W\"+ str(l+1)] = parameters[\"W\"+ str(l+1)] - learning_rate* grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\"+ str(l+1)] = parameters[\"b\"+ str(l+1)] - learning_rate* grads[\"db\"+str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.59562069, -0.09991781, -2.14584584,  1.82662008],\n",
       "        [-1.76569676, -0.80627147,  0.51115557, -1.18258802],\n",
       "        [-1.0535704 , -0.86128581,  0.68284052,  2.20374577]]),\n",
       " 'b1': array([[-0.04659241],\n",
       "        [-1.28888275],\n",
       "        [ 0.53405496]]),\n",
       " 'W2': array([[-0.55569196,  0.0354055 ,  1.32964895]]),\n",
       " 'b2': array([[-0.84610769]])}"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion . \n",
    "\n",
    "I have successfully implemented all the helper functions required to build a deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [3, 4, 2] \n",
    "#as we grow our neural network in terms of the number of layers we get huge number of parameters to train.\n",
    "# this is why big models can take very very long to train. as told can be days or months in coding ninjas class\n",
    "\n",
    "parameters = {}\n",
    "L = len(layer_dims) # number of layers in our neural network, including the input layer.\n",
    "for l in range(1,len(layer_dims)): #easiest way to get the parameters initialised for a L layer neural network\n",
    "    \n",
    "    parameters[\"W\"+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "    parameters[\"b\"+str(l)] = np.zeros((layer_dims[l],1))\n",
    "    \n",
    "    #dimension checking is paramount here.\n",
    "    \n",
    "    assert(parameters[\"W\"+ str(l)].shape ==(layer_dims[l], layer_dims[l-1]))\n",
    "                                            #gives n[l]  , gives n[l-1]\n",
    "    \n",
    "    assert(parameters[\"b\"+ str(l)].shape ==(layer_dims[l],1))\n",
    "                                           #gives n[l]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 [[ 0.01976111 -0.01244123 -0.00626417 -0.00803766]\n",
      " [-0.02419083 -0.00923792 -0.01023876  0.01123978]] \n",
      "\n",
      "W1 [[-0.01101068 -0.01185047 -0.0020565 ]\n",
      " [ 0.01486148  0.00236716 -0.01023785]\n",
      " [-0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W2\",parameters[\"W2\"],'\\n')#look how many parameters we got for a 4 layer neural network.\n",
    "print(\"W1\",parameters[\"W1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "L = 6\n",
    "for i in reversed(range (L-1)): # a loop from L-2 to 0\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = ((2,1,2,3), (4,9,98))\n",
    "linear_cache, activation_cache = cache #this is called unpacking. unpacking values contained in the cache tuple into two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1, 2, 3), (4, 9, 98))"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_cache, activation_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
